{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "This section will cover the basic things we do after we have acquired raw text from various sources\n",
    "\n",
    "- Using Strings\n",
    "- Using Unicode Strings\n",
    "- Regular Expressions\n",
    "- Normalization with Stemmers and Lemmatizers\n",
    "- Tokenization\n",
    "- Segmentation\n",
    "- Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Sumer's dat?Thow are more lovely and more temperate:\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single line\n",
    "circus = \"Monty Python's flying circus\"\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Sumer's dat?Thow are more lovely and more temperate:\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi line\n",
    "couplet = \"Shall I compare thee to a Sumer's dat?\"\\\n",
    "    \"Thow are more lovely and more temperate:\"\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veryveryvery'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenation\n",
    "'very' + 'very' + 'very'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access individual characters\n",
    "circus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loops\n",
    "for char in circus:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lying'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substrings\n",
    "circus[-12:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found it\n"
     ]
    }
   ],
   "source": [
    "phrase = 'And now for something completely different'\n",
    "if 'thing' in phrase:\n",
    "    print('found it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substring with find\n",
    "\"\"\"\n",
    "Others: find(t), rfind(t), index(t), rindex(t), join(text), split(t), splitlines(), lower(), upper(), titlecase(), strip(), replace(t)\n",
    "\"\"\" \n",
    "circus.find('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Define', 'which', 'data', 'represents', '\"', 'ham', '\"', 'class', 'and', 'which', 'data', 'represents', '\"', 'spam', '\"', 'class', 'for', 'the', 'machine-learning', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# split word\n",
    "text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine-learning algorithm.'\n",
    "# text = \"i. e.\"\n",
    "delimiters = ['\"', \".\"]\n",
    "words = []\n",
    "current_word = \"\" \n",
    "for char in text:\n",
    "    if char==\" \":\n",
    "        if not current_word==\"\":\n",
    "            words.append(current_word)\n",
    "            current_word = \"\" \n",
    "    elif char in delimiters:\n",
    "        if current_word==\"\":\n",
    "            words.append(char)\n",
    "        else:\n",
    "            words.append(current_word)\n",
    "            words.append(char)\n",
    "            current_word = \"\" \n",
    "    else:\n",
    "        current_word += char\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Unicode Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "\n",
    "wordlist = [w for w in corpus.words.words('en') if w.islower()] \n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization with Stemmers and Lemmatizers\n",
    "\n",
    "Normalization makes text ready for processing by removing punctuation, converting to lower or upper case, changing numbers to words, expanding of abbreviations and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'was', 'a', 'great', 'show'], ['Artists', 'acted'], ['Dancers', 'performed']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = ['It was a great show','Artists acted','Dancers performed']\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]\n",
    "x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text or sentence into smaller parts called tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome, I hope the journey was fine?', 'Please stay safe!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "\n",
    "text = 'Welcome, I hope the journey was fine? Please stay safe!'\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with large number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'Hope you are doing well.', 'Good to know you are']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = 'Hello everyone. Hope you are doing well. Good to know you are'\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage  collège franco-britanniquedeLevallois-Perret.', 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage  Levallois.', 'L’équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l’agression, janvier , d’un professeur d’histoire.', 'L’équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l’agression, mercredi , d’un professeur d’histoire']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "french_tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(french_tokenizer.tokenize('Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage  collège franco-britanniquedeLevallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage  Levallois. L’équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l’agression, janvier , d’un professeur d’histoire. L’équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l’agression, mercredi , d’un professeur d’histoire'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbc0770ecd30f98561162093fd8abab0709c5060f2f81711c403a4cebee6fe13"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
