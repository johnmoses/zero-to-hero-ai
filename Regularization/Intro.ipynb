{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a practice that helps to mitigate the problem of overfitting. It introduces a penalty for more complex models, effectively reducing their complexity and encouraging the model to learn more generalized patterns. \n",
    "\n",
    "This method strikes a balance between underfitting and overfitting, where underfitting occurs when the model is too simple to capture the underlying trends in the data, leading to both training and validation accuracy being low.\n",
    "\n",
    "### Overfitting\n",
    "Overfitting is like memorizing answers for a test without understanding the concepts — the model performs well on the training data but fails to predict new data accurately.\n",
    "\n",
    "### Underfitting\n",
    "Underfitting, on the other hand, is like not studying enough, the model doesn’t learn enough from the training data, leading to poor performance on both the training and new data.\n",
    "\n",
    "### Bias\n",
    "Bias is when a model has a predetermined, often incorrect assumption about the data, leading to errors in predictions. It’s like wearing tinted glasses and seeing everything in a particular color.\n",
    "\n",
    "### Variance\n",
    "This occurs when a model is too sensitive to the specifics of the training data, including the noise. It’s like trying to adapt to every minor change in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Regularization\n",
    "\n",
    "### L2- Ridge Regularization\n",
    "L2 R. adds a penalty based on the square of the magnitude of the coefficients. Using the same analogy of dials, Ridge doesn’t turn any dials to zero but makes them smaller. It helps to reduce the complexity of the model by making the coefficients smaller\n",
    "\n",
    "### L1 — Lasso Regularizaton\n",
    "Lasso regression also adds a penalty term to the cost function, but slightly different. L1 regularization makes some coefficients zero, meaning the model will ignore those features. Ignoring the least important features helps emphasize the model’s essential features.\n",
    "\n",
    "### Elastic Net Regularization — L1 and L2 Regularization\n",
    "Elastic Net combines both L1 and L2 Regularization. It adds both penalties (absolute value and square of the coefficients) to the model. This approach blends the feature elimination from Lasso and the feature shrinkage from Ridge. Elastic Net is particularly useful when there are multiple features that are correlated with each other. It can maintain the balance between feature selection and feature shrinkage, making it a versatile choice for many models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "housing = pd.read_csv('..xdata/housing_data.csv')\n",
    "housing.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
