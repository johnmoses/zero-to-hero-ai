{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "Big Data is essentially data in a larger scale. As data is being generated, stored and transmitted every second by means of connected devices and systems round the world, the volume of data can easily scale to a very large dimension. \n",
    "\n",
    "In recent times we metion technologies like Hadoop, Spark, NoSQL and graph databases (Neo4j) when we talk about big data. The entire infrastructure may not be domiciled or localized in on machine or in one place, they may exist in clusters and datacenters scattered over many locations.\n",
    "\n",
    "## Features of Big Data\n",
    "\n",
    "1. Volume: Variety and Velocity\n",
    "2. Volume: Size of the data\n",
    "3. Variety: Different sources & formats\n",
    "4. Velocity: Speed of teh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `pyspark` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "In order to work with Spark, we have to first set up a SparkSession, an object with which we can interact with Apache Spark.\n",
    "\n",
    "We will use the builder method is used to set up an app which we name 'HelloWorldApp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/18 14:23:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"HelloWorldApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spark SQL, we create a dataframe which holds our `hello world` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql('SELECT \"hello world\" as c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|         c1|\n",
      "+-----------+\n",
      "|hello world|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../xdata/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+\n",
      "|_c0| _c1|_c2|       _c3|\n",
      "+---+----+---+----------+\n",
      "|  1|  31|2.5|1260759144|\n",
      "|  1|1029|3.0|1260759179|\n",
      "|  1|1061|3.0|1260759182|\n",
      "|  1|1129|2.0|1260759185|\n",
      "|  1|1172|4.0|1260759205|\n",
      "|  1|1263|2.0|1260759151|\n",
      "|  1|1287|2.0|1260759187|\n",
      "|  1|1293|2.0|1260759148|\n",
      "|  1|1339|3.5|1260759125|\n",
      "|  1|1343|2.0|1260759131|\n",
      "|  1|1371|2.5|1260759135|\n",
      "|  1|1405|1.0|1260759203|\n",
      "|  1|1953|4.0|1260759191|\n",
      "|  1|2105|4.0|1260759139|\n",
      "|  1|2150|3.0|1260759194|\n",
      "|  1|2193|2.0|1260759198|\n",
      "|  1|2294|2.0|1260759108|\n",
      "|  1|2455|2.5|1260759113|\n",
      "|  1|2968|1.0|1260759200|\n",
      "|  1|3671|3.0|1260759117|\n",
      "+---+----+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21aa380f16a0108e00536e3e7793513d8af019678eecf9873d21f81a20cdd033"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
